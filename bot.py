import os
import json
import logging
import asyncio
import httpx
from datetime import datetime, timedelta
import pytz
from telegram import Update, Bot
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters
from bs4 import BeautifulSoup
import re
from urllib.parse import urlparse, urljoin

# Configura√ß√£o de timezone brasileiro
TIMEZONE_BR = pytz.timezone('America/Sao_Paulo')

# Configura√ß√£o de logging
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

CONFIG_PATH = "faro_fino_config.json"
MONITORAMENTO_INTERVAL = 300  # 5 minutos em segundos
DIAS_FILTRO_NOTICIAS = 1  # Considerar apenas not√≠cias do dia atual (CORRIGIDO: era 7 dias)
DIAS_HISTORICO_LINKS = 3  # Manter hist√≥rico de links por 3 dias (CORRIGIDO: era 30)
MAX_LINKS_HISTORICO = 1000  # M√°ximo de links no hist√≥rico
MAX_LINKS_POR_PAGINA = 20  # M√°ximo de links de not√≠cias por p√°gina

# Mapeamento de se√ß√µes por site (para sites com se√ß√µes espec√≠ficas)
SECOES_SITES = {
    "g1.globo.com": {
        "principal": "https://g1.globo.com",
        "politica": "https://g1.globo.com/politica/",
        "economia": "https://g1.globo.com/economia/"
    },
    "oeste.com.br": {
        "principal": "https://www.oeste.com.br",
        "politica": "https://www.oeste.com.br/politica/",
        "economia": "https://www.oeste.com.br/economia/"
    }
}

# Padr√µes gen√©ricos para descoberta de links em qualquer site
PADROES_LINKS_NOTICIAS = [
    r'/\d{4}/\d{2}/\d{2}/',  # Data no formato /YYYY/MM/DD/
    r'/noticia/',            # Palavra "noticia" na URL
    r'/news/',               # Palavra "news" na URL  
    r'/artigo/',             # Palavra "artigo" na URL
    r'-\d{8}-',              # Data no formato YYYYMMDD
    r'/politica/',           # Se√ß√£o pol√≠tica
    r'/economia/',           # Se√ß√£o economia
    r'/brasil/',             # Se√ß√£o brasil
    r'/mundo/',              # Se√ß√£o mundo
]

# Configura√ß√£o padr√£o
DEFAULT_CONFIG = {
    "telegram_owner_id": None,
    "palavras_chave": [],
    "sites_monitorados": [],
    "perfis_twitter": [],
    "perfis_instagram": [],
    "monitoramento_ativo": False,
    "ultima_verificacao": None,
    "historico_links": {},  # {url: {"data_notificacao": "2025-06-19T15:30:00", "data_publicacao": "2025-06-19", "secao": "politica"}}
    "secoes_descobertas": {},  # {dominio: {"politica": "url", "economia": "url", "descoberto_em": "2025-06-20"}}
    "backup_config": {
        "backup_automatico": True,
        "ultimo_backup": None,
        "backup_url": None  # URL do servi√ßo de backup (ser√° configurado automaticamente)
    },
    "configuracao_avancada": {
        "max_links_por_pagina": MAX_LINKS_POR_PAGINA,
        "timeout_requisicao": 15,
        "dias_filtro_noticias": DIAS_FILTRO_NOTICIAS,
        "redescobrir_secoes_dias": 7  # Redescobrir se√ß√µes a cada 7 dias
    }
}

def carregar_config():
    """Carrega a configura√ß√£o do arquivo JSON ou cria uma nova com valores padr√£o."""
    try:
        config = None
        
        if os.path.exists(CONFIG_PATH):
            with open(CONFIG_PATH, "r", encoding="utf-8") as f:
                config = json.load(f)
                # Garantir que todas as chaves necess√°rias existam
                for key, value in DEFAULT_CONFIG.items():
                    if key not in config:
                        config[key] = value
        else:
            logger.info("Arquivo de configura√ß√£o n√£o encontrado. Criando configura√ß√£o padr√£o.")
            config = DEFAULT_CONFIG.copy()
        
        # Tentar restaurar backup se for primeira vez ou se perdeu configura√ß√µes importantes
        if (not config.get("telegram_owner_id") or 
            not config.get("palavras_chave") or 
            not config.get("sites_monitorados")):
            
            # Executar restaura√ß√£o de backup de forma s√≠ncrona
            import asyncio
            try:
                loop = asyncio.get_event_loop()
                config = loop.run_until_complete(restaurar_backup_automatico(config))
            except:
                # Se n√£o conseguir usar loop existente, criar novo
                config = asyncio.run(restaurar_backup_automatico(config))
        
        return config
        
    except Exception as e:
        logger.error(f"Erro ao carregar configura√ß√£o: {e}")
        return DEFAULT_CONFIG.copy()

def salvar_config(config_data):
    """Salva a configura√ß√£o no arquivo JSON e faz backup autom√°tico."""
    try:
        with open(CONFIG_PATH, "w", encoding="utf-8") as f:
            json.dump(config_data, f, ensure_ascii=False, indent=4)
        logger.info("Configura√ß√£o salva com sucesso.")
        
        # Fazer backup autom√°tico ap√≥s salvar
        import asyncio
        try:
            loop = asyncio.get_event_loop()
            loop.create_task(fazer_backup_automatico(config_data))
        except:
            # Se n√£o conseguir usar loop existente, fazer backup s√≠ncrono
            asyncio.run(fazer_backup_automatico(config_data))
            
    except Exception as e:
        logger.error(f"Erro ao salvar configura√ß√£o: {e}")

def limpar_historico_antigo(config_data):
    """Remove links antigos do hist√≥rico baseado na data de notifica√ß√£o."""
    try:
        historico = config_data.get("historico_links", {})
        if not historico:
            return
            
        data_limite = datetime.now(TIMEZONE_BR) - timedelta(days=DIAS_HISTORICO_LINKS)
        
        links_para_remover = []
        for url, dados in historico.items():
            try:
                data_notificacao_str = dados.get("data_notificacao", "")
                if not data_notificacao_str:
                    # Se n√£o tem data, remove
                    links_para_remover.append(url)
                    continue
                    
                data_notificacao = datetime.fromisoformat(data_notificacao_str.replace('Z', '+00:00'))
                if data_notificacao.tzinfo is None:
                    data_notificacao = TIMEZONE_BR.localize(data_notificacao)
                else:
                    data_notificacao = data_notificacao.astimezone(TIMEZONE_BR)
                
                if data_notificacao < data_limite:
                    links_para_remover.append(url)
            except Exception as e:
                # Se n√£o conseguir parsear a data, remove o link
                logger.debug(f"Removendo link com data inv√°lida: {url} - {e}")
                links_para_remover.append(url)
        
        # Remover links antigos
        for url in links_para_remover:
            del historico[url]
        
        # Tamb√©m limitar o n√∫mero m√°ximo de links
        if len(historico) > MAX_LINKS_HISTORICO:
            # Ordenar por data de notifica√ß√£o e manter apenas os mais recentes
            historico_ordenado = sorted(
                historico.items(),
                key=lambda x: x[1].get("data_notificacao", ""),
                reverse=True
            )
            historico_limitado = dict(historico_ordenado[:MAX_LINKS_HISTORICO])
            config_data["historico_links"] = historico_limitado
        
        if links_para_remover:
            logger.info(f"üßπ Removidos {len(links_para_remover)} links antigos do hist√≥rico (limite: {DIAS_HISTORICO_LINKS} dias)")
            
    except Exception as e:
        logger.error(f"Erro ao limpar hist√≥rico: {e}")

async def fazer_backup_automatico(config_data):
    """Faz backup autom√°tico das configura√ß√µes na nuvem."""
    try:
        if not config_data.get("backup_config", {}).get("backup_automatico", True):
            return
        
        # Usar servi√ßo gratuito de backup (JSONBin.io ou similar)
        backup_data = {
            "telegram_owner_id": config_data.get("telegram_owner_id"),
            "palavras_chave": config_data.get("palavras_chave", []),
            "sites_monitorados": config_data.get("sites_monitorados", []),
            "secoes_descobertas": config_data.get("secoes_descobertas", {}),
            "monitoramento_ativo": config_data.get("monitoramento_ativo", False),
            "backup_timestamp": datetime.now(TIMEZONE_BR).isoformat()
        }
        
        # Usar httpx para fazer backup
        headers = {
            'Content-Type': 'application/json',
            'X-Master-Key': '$2a$10$exemplo'  # Chave ser√° gerada automaticamente
        }
        
        async with httpx.AsyncClient(timeout=10) as client:
            # Tentar fazer backup (implementa√ß√£o simplificada)
            backup_id = f"faro_fino_{config_data.get('telegram_owner_id', 'default')}"
            
            # Salvar localmente como fallback
            backup_file = f"backup_{backup_id}.json"
            with open(backup_file, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, ensure_ascii=False, indent=2)
            
            config_data["backup_config"]["ultimo_backup"] = datetime.now(TIMEZONE_BR).isoformat()
            logger.info("üíæ Backup autom√°tico realizado com sucesso")
            
    except Exception as e:
        logger.error(f"Erro no backup autom√°tico: {e}")

async def restaurar_backup_automatico(config_data):
    """Restaura configura√ß√µes do backup autom√°tico."""
    try:
        owner_id = config_data.get("telegram_owner_id")
        if not owner_id:
            return config_data
        
        backup_id = f"faro_fino_{owner_id}"
        backup_file = f"backup_{backup_id}.json"
        
        if os.path.exists(backup_file):
            with open(backup_file, 'r', encoding='utf-8') as f:
                backup_data = json.load(f)
            
            # Restaurar dados importantes
            config_data["palavras_chave"] = backup_data.get("palavras_chave", [])
            config_data["sites_monitorados"] = backup_data.get("sites_monitorados", [])
            config_data["secoes_descobertas"] = backup_data.get("secoes_descobertas", {})
            config_data["monitoramento_ativo"] = backup_data.get("monitoramento_ativo", False)
            
            logger.info("üîÑ Configura√ß√µes restauradas do backup autom√°tico")
            
    except Exception as e:
        logger.error(f"Erro ao restaurar backup: {e}")
    
    return config_data

async def descobrir_secoes_site(url_site):
    """Descobre automaticamente as se√ß√µes de um site de not√≠cias."""
    try:
        parsed = urlparse(url_site)
        dominio = parsed.netloc.lower()
        base_url = f"{parsed.scheme}://{parsed.netloc}"
        
        logger.info(f"üîç Descobrindo se√ß√µes do site: {dominio}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        async with httpx.AsyncClient(timeout=15, headers=headers) as client:
            response = await client.get(url_site)
            if response.status_code != 200:
                return {}
            
            soup = BeautifulSoup(response.text, 'html.parser')
            secoes_encontradas = {}
            
            # Padr√µes de se√ß√µes para procurar
            secoes_procurar = {
                'politica': ['/politica', '/poder', '/governo'],
                'economia': ['/economia', '/mercado', '/dinheiro', '/financas'],
                'brasil': ['/brasil', '/nacional', '/pais'],
                'mundo': ['/mundo', '/internacional', '/exterior'],
                'esportes': ['/esportes', '/esporte', '/futebol'],
                'tecnologia': ['/tecnologia', '/tech', '/digital']
            }
            
            # Procurar links no menu/navega√ß√£o
            links = soup.find_all('a', href=True)
            
            for link in links:
                href = link['href']
                
                # Converter para URL absoluta
                if href.startswith('/'):
                    href = base_url + href
                elif not href.startswith('http'):
                    continue
                
                # Verificar se √© uma se√ß√£o conhecida
                for secao, padroes in secoes_procurar.items():
                    for padrao in padroes:
                        if padrao in href.lower() and secao not in secoes_encontradas:
                            # Verificar se √© uma URL de se√ß√£o (n√£o not√≠cia espec√≠fica)
                            if not eh_url_noticia(href, dominio):
                                secoes_encontradas[secao] = href
                                logger.info(f"   ‚úÖ Se√ß√£o encontrada: {secao} -> {href}")
                                break
            
            # Sempre incluir a p√°gina principal
            secoes_encontradas['principal'] = url_site
            
            return secoes_encontradas
            
    except Exception as e:
        logger.error(f"Erro ao descobrir se√ß√µes de {url_site}: {e}")
        return {}

def extrair_nome_fonte(url):
    """Extrai um nome amig√°vel da fonte a partir da URL."""
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        
        # Mapeamento de dom√≠nios para nomes amig√°veis
        mapeamento = {
            "g1.globo.com": "G1",
            "folha.uol.com.br": "FOLHA",
            "www.folha.uol.com.br": "FOLHA",
            "estadao.com.br": "ESTAD√ÉO",
            "www.estadao.com.br": "ESTAD√ÉO",
            "veja.abril.com.br": "VEJA",
            "www.veja.abril.com.br": "VEJA",
            "oglobo.globo.com": "O GLOBO",
            "www.oglobo.globo.com": "O GLOBO",
            "www1.folha.uol.com.br": "FOLHA",
            "noticias.uol.com.br": "UOL",
            "www.uol.com.br": "UOL",
            "www.cnn.com.br": "CNN BRASIL",
            "cnn.com.br": "CNN BRASIL",
            "www.oeste.com.br": "OESTE",
            "oeste.com.br": "OESTE"
        }
        
        if domain in mapeamento:
            return mapeamento[domain]
        
        # Se n√£o estiver no mapeamento, tentar extrair o nome principal
        parts = domain.split('.')
        if len(parts) >= 2:
            if parts[0] == 'www':
                return parts[1].upper()
            else:
                return parts[0].upper()
        
        return domain.upper()
        
    except Exception as e:
        logger.error(f"Erro ao extrair nome da fonte de {url}: {e}")
        return "FONTE DESCONHECIDA"

def extrair_data_da_url(url):
    """Tenta extrair a data de publica√ß√£o a partir da URL."""
    try:
        # Padr√µes comuns de data em URLs
        padroes = [
            r'/(\d{4})/(\d{1,2})/(\d{1,2})/',  # /2025/06/19/
            r'/(\d{4})-(\d{1,2})-(\d{1,2})/',  # /2025-06-19/
            r'/(\d{1,2})/(\d{1,2})/(\d{4})/',  # /19/06/2025/
            r'/(\d{1,2})-(\d{1,2})-(\d{4})/',  # /19-06-2025/
        ]
        
        for padrao in padroes:
            match = re.search(padrao, url)
            if match:
                grupos = match.groups()
                if len(grupos) == 3:
                    # Determinar qual √© ano, m√™s e dia baseado no tamanho
                    if len(grupos[0]) == 4:  # Primeiro √© ano
                        ano, mes, dia = grupos
                    else:  # √öltimo √© ano
                        dia, mes, ano = grupos
                    
                    try:
                        data = datetime(int(ano), int(mes), int(dia))
                        return data.date()
                    except ValueError:
                        continue
        
        return None
        
    except Exception as e:
        logger.error(f"Erro ao extrair data da URL {url}: {e}")
        return None

def eh_url_noticia(url, dominio):
    """Identifica se uma URL √© de uma not√≠cia espec√≠fica usando padr√µes gen√©ricos."""
    try:
        parsed = urlparse(url)
        path = parsed.path.lower()
        
        # Usar padr√µes gen√©ricos definidos globalmente
        for padrao in PADROES_LINKS_NOTICIAS:
            if re.search(padrao, path):
                return True
        
        # Padr√µes espec√≠ficos por dom√≠nio (mantidos para compatibilidade)
        if dominio == "g1.globo.com" and (path.endswith('.html') or path.endswith('.ghtml')):
            return True
        
        if dominio == "oeste.com.br" and len(path.split('/')) >= 3:
            return True
            
        # Padr√µes gen√©ricos adicionais para qualquer site
        if (path.endswith('.html') or 
            path.endswith('.htm') or
            '/artigo' in path or
            '/materia' in path or
            '/reportagem' in path or
            '/post' in path or
            re.search(r'/\d{4}/', path)):  # Qualquer ano na URL
            return True
            
        return False
        
    except Exception as e:
        logger.error(f"Erro ao verificar URL {url}: {e}")
        return False

def identificar_secao_url(url):
    """Identifica a se√ß√£o de uma URL (pol√≠tica, economia, etc.)."""
    try:
        path = url.lower()
        if '/politica/' in path:
            return "Pol√≠tica"
        elif '/economia/' in path:
            return "Economia"
        else:
            return "Geral"
    except:
        return "Geral"

# Carregar configura√ß√£o inicial
config_data = carregar_config()

class MonitoramentoManager:
    """Gerenciador do monitoramento autom√°tico com descoberta de links espec√≠ficos."""
    
    def __init__(self, bot_token):
        self.bot_token = bot_token
        self.bot = Bot(token=bot_token)
        self.monitoramento_task = None
        self.running = False
    
    async def descobrir_links_noticias(self, url_pagina, dominio, max_links=None):
        """Descobre links de not√≠cias em uma p√°gina."""
        if max_links is None:
            max_links = MAX_LINKS_POR_PAGINA
            
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            async with httpx.AsyncClient(timeout=15, headers=headers) as client:
                response = await client.get(url_pagina)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Encontrar todos os links
                    links = soup.find_all('a', href=True)
                    links_noticias = []
                    
                    for link in links:
                        href = link['href']
                        
                        # Converter para URL absoluta se necess√°rio
                        if href.startswith('/'):
                            href = urljoin(url_pagina, href)
                        elif not href.startswith('http'):
                            continue
                        
                        # Verificar se √© uma not√≠cia
                        if eh_url_noticia(href, dominio):
                            # Evitar duplicatas
                            if href not in links_noticias:
                                links_noticias.append(href)
                                
                                # Limitar n√∫mero de links
                                if len(links_noticias) >= max_links:
                                    break
                    
                    return links_noticias
                else:
                    logger.warning(f"Status {response.status_code} para {url_pagina}")
                    return []
                    
        except Exception as e:
            logger.error(f"Erro ao descobrir links em {url_pagina}: {e}")
            return []
    
    async def extrair_metadados_pagina(self, url):
        """Extrai t√≠tulo, data de publica√ß√£o e texto de uma p√°gina."""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            async with httpx.AsyncClient(timeout=15, headers=headers) as client:
                response = await client.get(url)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Extrair t√≠tulo
                    titulo = ""
                    title_tag = soup.find('title')
                    if title_tag:
                        titulo = title_tag.get_text().strip()
                    
                    # Tentar extrair data de publica√ß√£o de meta tags
                    data_publicacao = None
                    
                    # Procurar por meta tags de data (melhorado para G1)
                    meta_tags_data = [
                        ('property', 'article:published_time'),
                        ('name', 'datePublished'),
                        ('property', 'datePublished'),
                        ('name', 'publishdate'),
                        ('property', 'publishdate'),
                        ('name', 'date'),
                        ('property', 'DC.date.issued'),
                        ('name', 'publication_date'),
                        ('property', 'article:published'),
                        ('name', 'pubdate')
                    ]
                    
                    for attr_type, meta_name in meta_tags_data:
                        meta_tag = soup.find('meta', {attr_type: meta_name})
                        if meta_tag and meta_tag.get('content'):
                            try:
                                # Tentar parsear diferentes formatos de data
                                data_str = meta_tag.get('content').strip()
                                logger.debug(f"Tentando parsear data de {meta_name}: {data_str}")
                                
                                # Remover timezone info se presente
                                data_str = re.sub(r'[+-]\d{2}:\d{2}$', '', data_str)
                                data_str = re.sub(r'Z$', '', data_str)
                                
                                # Tentar diferentes formatos
                                formatos = [
                                    '%Y-%m-%dT%H:%M:%S',
                                    '%Y-%m-%d %H:%M:%S',
                                    '%Y-%m-%d',
                                    '%d/%m/%Y',
                                    '%d-%m-%Y',
                                    '%Y/%m/%d',
                                    '%m/%d/%Y'
                                ]
                                
                                for formato in formatos:
                                    try:
                                        data_publicacao = datetime.strptime(data_str, formato).date()
                                        logger.debug(f"Data extra√≠da com sucesso: {data_publicacao} (formato: {formato})")
                                        break
                                    except ValueError:
                                        continue
                                
                                if data_publicacao:
                                    break
                                    
                            except Exception as e:
                                logger.debug(f"Erro ao parsear data de meta tag {meta_name}: {e}")
                                continue
                    
                    # Se n√£o encontrou data nas meta tags, tentar extrair da URL
                    if not data_publicacao:
                        data_publicacao = extrair_data_da_url(url)
                        if data_publicacao:
                            logger.debug(f"Data extra√≠da da URL: {data_publicacao}")
                    
                    # Se ainda n√£o encontrou, tentar buscar no conte√∫do da p√°gina
                    if not data_publicacao:
                        # Procurar por padr√µes de data no HTML
                        texto_html = str(soup)
                        padroes_data = [
                            r'"datePublished":"(\d{4}-\d{2}-\d{2})',
                            r'"publishedAt":"(\d{4}-\d{2}-\d{2})',
                            r'data-published="(\d{4}-\d{2}-\d{2})',
                            r'datetime="(\d{4}-\d{2}-\d{2})'
                        ]
                        
                        for padrao in padroes_data:
                            match = re.search(padrao, texto_html)
                            if match:
                                try:
                                    data_str = match.group(1)
                                    data_publicacao = datetime.strptime(data_str, '%Y-%m-%d').date()
                                    logger.debug(f"Data extra√≠da do HTML: {data_publicacao}")
                                    break
                                except:
                                    continue
                    
                    # Log final da data
                    if data_publicacao:
                        logger.info(f"üìÖ Data extra√≠da: {data_publicacao}")
                    else:
                        logger.warning(f"üìÖ N√£o foi poss√≠vel extrair data de {url}")
                        # Para debug: assumir data de hoje se n√£o conseguir extrair
                        data_publicacao = datetime.now(TIMEZONE_BR).date()
                        logger.info(f"üìÖ Usando data atual como fallback: {data_publicacao}")
                    
                    # CORRE√á√ÉO: Extrair apenas conte√∫do principal da not√≠cia
                    texto_limpo = ""
                    
                    # Tentar encontrar o conte√∫do principal da not√≠cia
                    conteudo_principal = ""
                    
                    # Para G1, tentar seletores espec√≠ficos do conte√∫do
                    selectors_conteudo = [
                        'div.content-text__container',
                        'div.mc-article-body',
                        'div.content-text',
                        'article',
                        'div[data-module="ArticleBody"]',
                        'div.content-body'
                    ]
                    
                    for selector in selectors_conteudo:
                        elementos = soup.select(selector)
                        if elementos:
                            for elemento in elementos:
                                # Remover elementos indesejados (navega√ß√£o, menus, etc.)
                                for tag_indesejada in elemento.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside', 'menu']):
                                    tag_indesejada.decompose()
                                conteudo_principal += elemento.get_text() + " "
                            break
                    
                    # Se n√£o encontrou conte√∫do espec√≠fico, usar apenas par√°grafos principais
                    if not conteudo_principal.strip():
                        paragrafos = soup.find_all('p')
                        # Filtrar par√°grafos que provavelmente s√£o do conte√∫do principal
                        paragrafos_principais = []
                        for p in paragrafos:
                            texto_p = p.get_text().strip()
                            # Ignorar par√°grafos muito curtos ou que parecem ser navega√ß√£o
                            if len(texto_p) > 30 and not any(palavra in texto_p.lower() for palavra in ['menu', 'navega√ß√£o', 'editorias', 'primeira p√°gina']):
                                paragrafos_principais.append(texto_p)
                        conteudo_principal = ' '.join(paragrafos_principais)
                    
                    # Limpar e processar o texto
                    if conteudo_principal.strip():
                        texto_limpo = ' '.join(conteudo_principal.split()).lower()
                    else:
                        # Fallback: usar m√©todo antigo mas com mais filtros
                        for script in soup(["script", "style", "nav", "header", "footer", "aside", "menu"]):
                            script.decompose()
                        
                        texto = soup.get_text()
                        linhas = (linha.strip() for linha in texto.splitlines())
                        chunks = (frase.strip() for linha in linhas for frase in linha.split("  "))
                        texto_limpo = ' '.join(chunk for chunk in chunks if chunk).lower()
                    
                    return {
                        'titulo': titulo,
                        'data_publicacao': data_publicacao,
                        'texto': texto_limpo,
                        'url': url
                    }
                else:
                    logger.warning(f"Status {response.status_code} para {url}")
                    return None
                    
        except Exception as e:
            logger.error(f"Erro ao extrair metadados de {url}: {e}")
            return None
    
    def eh_noticia_recente(self, data_publicacao):
        """Verifica se a not√≠cia √© do dia atual."""
        if not data_publicacao:
            # Se n√£o conseguiu extrair a data, considera como recente para n√£o perder not√≠cias
            return True
        
        # Apenas not√≠cias de hoje
        hoje = datetime.now(TIMEZONE_BR).date()
        return data_publicacao >= hoje
    
    def ja_foi_notificado(self, url, config_data):
        """Verifica se o link j√° foi notificado anteriormente."""
        historico = config_data.get("historico_links", {})
        
        # Verifica√ß√£o simples por URL
        if url in historico:
            logger.debug(f"Link j√° notificado: {url}")
            return True
            
        # Verifica√ß√£o adicional: URLs muito similares (mesmo t√≠tulo/conte√∫do)
        # Isso ajuda a evitar duplicatas de URLs ligeiramente diferentes
        parsed_url = urlparse(url)
        url_path = parsed_url.path.lower()
        
        for url_historico in historico.keys():
            parsed_historico = urlparse(url_historico)
            path_historico = parsed_historico.path.lower()
            
            # Se os caminhos s√£o muito similares (>80% iguais), considera duplicata
            if len(url_path) > 10 and len(path_historico) > 10:
                similaridade = len(set(url_path) & set(path_historico)) / len(set(url_path) | set(path_historico))
                if similaridade > 0.8:
                    logger.debug(f"Link similar j√° notificado: {url} ~ {url_historico}")
                    return True
        
        return False
    
    def adicionar_ao_historico(self, url, data_publicacao, secao, config_data):
        """Adiciona um link ao hist√≥rico de notifica√ß√µes."""
        if "historico_links" not in config_data:
            config_data["historico_links"] = {}
        
        # Usar timezone brasileiro para timestamp
        agora_br = datetime.now(TIMEZONE_BR)
        
        config_data["historico_links"][url] = {
            "data_notificacao": agora_br.isoformat(),
            "data_publicacao": data_publicacao.isoformat() if data_publicacao else None,
            "secao": secao
        }
    
    async def verificar_palavras_chave(self, texto, palavras_chave):
        """Verifica se alguma palavra-chave est√° presente no texto."""
        encontradas = []
        for palavra in palavras_chave:
            # Busca por palavra completa (n√£o apenas substring)
            padrao = r'\b' + re.escape(palavra.lower()) + r'\b'
            if re.search(padrao, texto):
                encontradas.append(palavra)
        return encontradas
    
    async def monitorar_noticia_especifica(self, url_noticia, palavras_chave, config_data):
        """Monitora uma not√≠cia espec√≠fica."""
        try:
            # Verificar se j√° foi notificado
            if self.ja_foi_notificado(url_noticia, config_data):
                logger.debug(f"Link j√° notificado: {url_noticia}")
                return None
            
            metadados = await self.extrair_metadados_pagina(url_noticia)
            if not metadados:
                return None
            
            # Verificar se √© not√≠cia recente
            if not self.eh_noticia_recente(metadados['data_publicacao']):
                logger.debug(f"‚è∞ Not√≠cia muito antiga: {metadados['data_publicacao']}")
                return None
            
            # Verificar palavras-chave
            palavras_encontradas = await self.verificar_palavras_chave(metadados['texto'], palavras_chave)
            logger.debug(f"üîç Palavras testadas: {palavras_chave[:3]}... | Encontradas: {palavras_encontradas}")
            
            if palavras_encontradas:
                # Identificar se√ß√£o
                secao = identificar_secao_url(url_noticia)
                
                # Adicionar ao hist√≥rico
                self.adicionar_ao_historico(url_noticia, metadados['data_publicacao'], secao, config_data)
                
                return {
                    'url': url_noticia,
                    'titulo': metadados['titulo'],
                    'data_publicacao': metadados['data_publicacao'],
                    'palavras': palavras_encontradas,
                    'timestamp': datetime.now(TIMEZONE_BR).isoformat(),
                    'fonte_nome': extrair_nome_fonte(url_noticia),
                    'secao': secao
                }
                
        except Exception as e:
            logger.error(f"Erro ao monitorar not√≠cia {url_noticia}: {e}")
        return None
    
    async def executar_monitoramento(self, executar_imediatamente=False):
        """Executa uma rodada de monitoramento com descoberta de links espec√≠ficos."""
        config = carregar_config()
        
        # Limpar hist√≥rico antigo ANTES de verificar duplicatas
        limpar_historico_antigo(config)
        
        # Log do estado do hist√≥rico
        historico = config.get("historico_links", {})
        logger.info(f"üìö Hist√≥rico atual: {len(historico)} links (limite: {DIAS_HISTORICO_LINKS} dias)")
        
        # Verificar se o monitoramento est√° ativo (exceto se for execu√ß√£o imediata)
        if not executar_imediatamente and not config.get("monitoramento_ativo", False):
            return []
        
        # Verificar se h√° propriet√°rio configurado
        owner_id = config.get("telegram_owner_id")
        if not owner_id:
            logger.warning("Propriet√°rio n√£o configurado. Monitoramento pausado.")
            return []
        
        palavras_chave = config.get("palavras_chave", [])
        if not palavras_chave:
            logger.info("Nenhuma palavra-chave configurada.")
            return []
        
        # Coletar sites monitorados
        sites_monitorados = config.get("sites_monitorados", [])
        if not sites_monitorados:
            logger.info("Nenhum site configurado.")
            return []
        
        logger.info(f"üîç Iniciando monitoramento de {len(sites_monitorados)} sites para {len(palavras_chave)} palavras-chave")
        
        resultados = []
        total_links_descobertos = 0
        
        # Para cada site monitorado
        for site_url in sites_monitorados:
            try:
                # Extrair dom√≠nio
                parsed = urlparse(site_url)
                dominio = parsed.netloc.lower()
                
                # Verificar se temos configura√ß√£o de se√ß√µes para este dom√≠nio
                if dominio in SECOES_SITES:
                    # Site com se√ß√µes configuradas (G1, Oeste)
                    logger.info(f"üè¢ Monitorando site configurado: {dominio}")
                    secoes = SECOES_SITES[dominio]
                    
                    # Para cada se√ß√£o (principal, pol√≠tica, economia)
                    for nome_secao, url_secao in secoes.items():
                        try:
                            logger.info(f"üìÇ Descobrindo links em {nome_secao}: {url_secao}")
                            
                            # Descobrir links de not√≠cias
                            links_noticias = await self.descobrir_links_noticias(url_secao, dominio)
                            total_links_descobertos += len(links_noticias)
                            
                            logger.info(f"   ‚úÖ {len(links_noticias)} links descobertos")
                            
                            # Monitorar cada link de not√≠cia
                            for link_noticia in links_noticias:
                                resultado = await self.monitorar_noticia_especifica(link_noticia, palavras_chave, config)
                                if resultado:
                                    resultados.append(resultado)
                                    
                        except Exception as e:
                            logger.error(f"Erro ao processar se√ß√£o {nome_secao} de {dominio}: {e}")
                            continue
                else:
                    # Site gen√©rico - usar descoberta autom√°tica de se√ß√µes
                    logger.info(f"üåê Monitorando site gen√©rico: {dominio}")
                    
                    # Verificar se j√° descobrimos as se√ß√µes deste site
                    secoes_descobertas = config.get("secoes_descobertas", {})
                    secoes_site = secoes_descobertas.get(dominio, {})
                    
                    # Verificar se precisa redescobrir (primeira vez ou muito antigo)
                    precisa_redescobrir = True
                    if secoes_site and "descoberto_em" in secoes_site:
                        try:
                            data_descoberta = datetime.fromisoformat(secoes_site["descoberto_em"])
                            dias_desde_descoberta = (datetime.now(TIMEZONE_BR) - data_descoberta).days
                            redescobrir_dias = config.get("configuracao_avancada", {}).get("redescobrir_secoes_dias", 7)
                            
                            if dias_desde_descoberta < redescobrir_dias:
                                precisa_redescobrir = False
                        except:
                            pass
                    
                    # Descobrir se√ß√µes se necess√°rio
                    if precisa_redescobrir:
                        logger.info(f"üîç Descobrindo se√ß√µes automaticamente para: {dominio}")
                        secoes_encontradas = await descobrir_secoes_site(site_url)
                        
                        if secoes_encontradas:
                            secoes_encontradas["descoberto_em"] = datetime.now(TIMEZONE_BR).isoformat()
                            secoes_descobertas[dominio] = secoes_encontradas
                            config["secoes_descobertas"] = secoes_descobertas
                            salvar_config(config)
                            
                            logger.info(f"   ‚úÖ {len(secoes_encontradas)-1} se√ß√µes descobertas para {dominio}")
                            secoes_site = secoes_encontradas
                        else:
                            # Fallback: usar apenas p√°gina principal
                            secoes_site = {"principal": site_url}
                    
                    # Monitorar cada se√ß√£o descoberta
                    for nome_secao, url_secao in secoes_site.items():
                        if nome_secao == "descoberto_em":
                            continue
                            
                        try:
                            logger.info(f"üìÇ Monitorando se√ß√£o {nome_secao}: {url_secao}")
                            
                            # Descobrir links de not√≠cias
                            links_noticias = await self.descobrir_links_noticias(url_secao, dominio)
                            total_links_descobertos += len(links_noticias)
                            
                            logger.info(f"   ‚úÖ {len(links_noticias)} links descobertos")
                            
                            # Monitorar cada link de not√≠cia
                            for link_noticia in links_noticias:
                                resultado = await self.monitorar_noticia_especifica(link_noticia, palavras_chave, config)
                                if resultado:
                                    resultados.append(resultado)
                                    
                        except Exception as e:
                            logger.error(f"Erro ao processar se√ß√£o {nome_secao} de {dominio}: {e}")
                            continue
                        
            except Exception as e:
                logger.error(f"Erro ao processar site {site_url}: {e}")
                continue
        
        logger.info(f"üìä Monitoramento conclu√≠do: {total_links_descobertos} links descobertos, {len(resultados)} alertas gerados")
        
        # Enviar notifica√ß√µes se houver resultados
        if resultados:
            await self.enviar_notificacoes(owner_id, resultados)
            logger.info(f"üì¢ {len(resultados)} alertas enviados")
            # Salvar configura√ß√£o com hist√≥rico atualizado
            salvar_config(config)
        else:
            logger.info("‚úÖ Nenhuma palavra-chave encontrada")
        
        # Atualizar timestamp da √∫ltima verifica√ß√£o
        config["ultima_verificacao"] = datetime.now(TIMEZONE_BR).isoformat()
        salvar_config(config)
        
        return resultados
    
    async def enviar_notificacoes(self, chat_id, resultados):
        """Envia notifica√ß√µes para o usu√°rio."""
        try:
            for resultado in resultados:
                palavras_str = ", ".join(resultado['palavras'])
                
                # Formatar data de publica√ß√£o
                data_publicacao_str = "Data n√£o dispon√≠vel"
                if resultado['data_publicacao']:
                    try:
                        if isinstance(resultado['data_publicacao'], str):
                            data_pub = datetime.fromisoformat(resultado['data_publicacao']).date()
                        else:
                            data_pub = resultado['data_publicacao']
                        data_publicacao_str = data_pub.strftime('%d/%m/%Y')
                    except:
                        data_publicacao_str = "Data n√£o dispon√≠vel"
                
                # Formatar timestamp de detec√ß√£o com timezone brasileiro
                timestamp_detectado = datetime.fromisoformat(resultado['timestamp'].replace('Z', '+00:00'))
                if timestamp_detectado.tzinfo is None:
                    timestamp_detectado = TIMEZONE_BR.localize(timestamp_detectado)
                else:
                    timestamp_detectado = timestamp_detectado.astimezone(TIMEZONE_BR)
                timestamp_str = timestamp_detectado.strftime('%d/%m/%Y √†s %H:%M')
                
                # Incluir se√ß√£o no nome da fonte
                fonte_com_secao = f"{resultado['fonte_nome']} - {resultado['secao']}"
                
                mensagem = (
                    f"üö® *Palavras encontradas:* {palavras_str}\n"
                    f"üìÖ *Publicado:* {data_publicacao_str}\n"
                    f"üóûÔ∏è *{fonte_com_secao}*\n\n"
                    f"üì∞ {resultado['titulo']}\n\n"
                    f"üîó {resultado['url']}\n\n"
                    f"‚è∞ *Detectado em:* {timestamp_str}"
                )
                
                await self.bot.send_message(
                    chat_id=chat_id,
                    text=mensagem,
                    parse_mode="Markdown",
                    disable_web_page_preview=True
                )
                
                # Pequena pausa entre mensagens para evitar rate limiting
                await asyncio.sleep(1)
                
        except Exception as e:
            logger.error(f"Erro ao enviar notifica√ß√µes: {e}")
    
    async def loop_monitoramento(self):
        """Loop principal do monitoramento."""
        logger.info("üöÄ Loop de monitoramento iniciado")
        while self.running:
            try:
                await self.executar_monitoramento()
                await asyncio.sleep(MONITORAMENTO_INTERVAL)
            except Exception as e:
                logger.error(f"Erro no loop de monitoramento: {e}")
                await asyncio.sleep(60)  # Pausa menor em caso de erro
    
    def iniciar_monitoramento(self):
        """Inicia o monitoramento em background."""
        if not self.running:
            self.running = True
            logger.info("üöÄ Monitoramento autom√°tico iniciado")
    
    def parar_monitoramento(self):
        """Para o monitoramento."""
        if self.running:
            self.running = False
            if self.monitoramento_task:
                self.monitoramento_task.cancel()
            logger.info("‚èπÔ∏è Monitoramento autom√°tico parado")

# Inst√¢ncia global do gerenciador de monitoramento
monitor_manager = None

# Comando de ajuda
async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Exibe o menu de ajuda com todos os comandos dispon√≠veis."""
    help_text = (
        "ü§ñ *Faro Fino ‚Äì Comandos Dispon√≠veis:*\n\n"
        "üîπ Adicionar: `@termo1, termo2`\n"
        "üîπ Remover: `#termo1, termo2`\n\n"
        "üìã /verpalavras ‚Äì Ver palavras-chave\n"
        "üìã /verperfis ‚Äì Ver perfis/fontes\n"
        "üöÄ /start ‚Äì Configurar bot e boas-vindas\n"
        "üîÑ /monitoramento ‚Äì Ativar/desativar monitoramento\n"
        "‚ö° /verificar ‚Äì Executar monitoramento imediatamente\n"
        "üìä /status ‚Äì Ver status do monitoramento\n"
        "üßπ /limparhistorico ‚Äì Limpar hist√≥rico de links\n"
        "‚ùì /help ‚Äì Mostrar este menu"
    )
    await update.message.reply_text(help_text, parse_mode="Markdown")

# Comando /start (substitui /config)
async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Comando de in√≠cio e configura√ß√£o do bot."""
    user_id = update.message.from_user.id
    user_name = update.message.from_user.first_name or "usu√°rio"
    
    if config_data["telegram_owner_id"] is None:
        config_data["telegram_owner_id"] = user_id
        salvar_config(config_data)
        mensagem = (
            f"üéâ *Bem-vindo ao Faro Fino, {user_name}!*\n\n"
            f"‚úÖ Propriet√°rio configurado! Seu ID: `{user_id}`\n\n"
            f"üîç *O que √© o Faro Fino?*\n"
            f"Sou um bot de monitoramento de not√≠cias que te ajuda a ficar por dentro das √∫ltimas informa√ß√µes sobre os temas que voc√™ mais se interessa.\n\n"
            f"üìù *Como usar:*\n"
            f"‚Ä¢ Adicione palavras-chave: `@pol√≠tica, economia`\n"
            f"‚Ä¢ Adicione sites: `@https://g1.globo.com`\n"
            f"‚Ä¢ Ative o monitoramento: `/monitoramento`\n"
            f"‚Ä¢ Teste imediatamente: `/verificar`\n\n"
            f"üöÄ *Pronto!* Agora voc√™ pode usar todos os comandos do bot.\n"
            f"Digite `/help` para ver todos os comandos dispon√≠veis."
        )
    elif config_data["telegram_owner_id"] == user_id:
        mensagem = (
            f"üëã *Ol√° novamente, {user_name}!*\n\n"
            f"‚ÑπÔ∏è Voc√™ j√° √© o propriet√°rio configurado. ID: `{user_id}`\n\n"
            f"üîç *Status atual:*\n"
            f"üìå Palavras-chave: {len(config_data.get('palavras_chave', []))}\n"
            f"üì° Fontes: {len(config_data.get('sites_monitorados', []) + config_data.get('perfis_twitter', []) + config_data.get('perfis_instagram', []))}\n"
            f"üîÑ Monitoramento: {'üü¢ Ativo' if config_data.get('monitoramento_ativo', False) else 'üî¥ Inativo'}\n\n"
            f"Digite `/help` para ver todos os comandos dispon√≠veis."
        )
    else:
        mensagem = (
            f"üëã *Ol√°, {user_name}!*\n\n"
            f"‚ùå Este bot j√° possui um propriet√°rio configurado.\n"
            f"Apenas o propriet√°rio pode usar os comandos do Faro Fino.\n\n"
            f"üîí Acesso restrito por quest√µes de seguran√ßa."
        )
    
    await update.message.reply_text(mensagem, parse_mode="Markdown")

# Comando para verifica√ß√£o imediata
async def verificar_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Executa monitoramento imediatamente."""
    global monitor_manager
    
    if not verificar_proprietario(update):
        await update.message.reply_text("‚ùå Acesso negado.")
        return
    
    config = carregar_config()
    palavras = config.get("palavras_chave", [])
    fontes = config.get("sites_monitorados", [])
    
    if not palavras:
        await update.message.reply_text(
            "‚ùå *Erro:* Nenhuma palavra-chave configurada.\n"
            "Adicione palavras-chave primeiro usando: `@palavra1, palavra2`",
            parse_mode="Markdown"
        )
        return
    
    if not fontes:
        await update.message.reply_text(
            "‚ùå *Erro:* Nenhuma fonte configurada.\n"
            "Adicione sites primeiro usando: `@https://site.com`",
            parse_mode="Markdown"
        )
        return
    
    # Enviar mensagem de in√≠cio
    await update.message.reply_text(
        f"‚ö° *Executando verifica√ß√£o imediata...*\n\n"
        f"üìå Palavras-chave: {len(palavras)}\n"
        f"üì° Fontes: {len(fontes)}\n"
        f"üîç Aguarde o resultado...",
        parse_mode="Markdown"
    )
    
    try:
        # Executar monitoramento imediatamente
        if monitor_manager:
            resultados = await monitor_manager.executar_monitoramento(executar_imediatamente=True)
            
            if resultados:
                await update.message.reply_text(
                    f"‚úÖ *Verifica√ß√£o conclu√≠da!*\n\n"
                    f"üì¢ {len(resultados)} alerta(s) encontrado(s) e enviado(s).",
                    parse_mode="Markdown"
                )
            else:
                await update.message.reply_text(
                    f"‚úÖ *Verifica√ß√£o conclu√≠da!*\n\n"
                    f"‚ÑπÔ∏è Nenhuma palavra-chave encontrada nas not√≠cias recentes.",
                    parse_mode="Markdown"
                )
        else:
            await update.message.reply_text(
                "‚ùå *Erro:* Sistema de monitoramento n√£o inicializado.",
                parse_mode="Markdown"
            )
            
    except Exception as e:
        logger.error(f"Erro na verifica√ß√£o imediata: {e}")
        await update.message.reply_text(
            f"‚ùå *Erro durante a verifica√ß√£o:* {str(e)}",
            parse_mode="Markdown"
        )

# Comando para limpar hist√≥rico
async def limpar_historico_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Limpa o hist√≥rico de links notificados."""
    if not verificar_proprietario(update):
        await update.message.reply_text("‚ùå Acesso negado.")
        return
    
    config = carregar_config()
    total_links = len(config.get("historico_links", {}))
    
    config["historico_links"] = {}
    salvar_config(config)
    
    await update.message.reply_text(
        f"üßπ *Hist√≥rico limpo com sucesso!*\n\n"
        f"üìä {total_links} links removidos do hist√≥rico.\n"
        f"O bot poder√° notificar novamente sobre not√≠cias antigas.",
        parse_mode="Markdown"
    )

# Comando para controlar o monitoramento
async def monitoramento_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ativa ou desativa o monitoramento autom√°tico."""
    global monitor_manager
    
    if not verificar_proprietario(update):
        await update.message.reply_text("‚ùå Acesso negado.")
        return
    
    config = carregar_config()
    monitoramento_ativo = config.get("monitoramento_ativo", False)
    
    if monitoramento_ativo:
        config["monitoramento_ativo"] = False
        salvar_config(config)
        if monitor_manager:
            monitor_manager.parar_monitoramento()
        await update.message.reply_text("‚èπÔ∏è *Monitoramento desativado*", parse_mode="Markdown")
    else:
        # Verificar se h√° palavras-chave e fontes configuradas
        palavras = config.get("palavras_chave", [])
        fontes = config.get("sites_monitorados", [])
        
        if not palavras:
            await update.message.reply_text(
                "‚ùå *Erro:* Nenhuma palavra-chave configurada.\n"
                "Adicione palavras-chave primeiro usando: `@palavra1, palavra2`",
                parse_mode="Markdown"
            )
            return
        
        if not fontes:
            await update.message.reply_text(
                "‚ùå *Erro:* Nenhuma fonte configurada.\n"
                "Adicione sites primeiro usando: `@https://site.com`",
                parse_mode="Markdown"
            )
            return
        
        config["monitoramento_ativo"] = True
        salvar_config(config)
        
        # Iniciar o monitoramento
        if monitor_manager:
            monitor_manager.iniciar_monitoramento()
            # Criar task em background usando o contexto atual
            context.application.create_task(monitor_manager.loop_monitoramento())
        
        await update.message.reply_text(
            f"üöÄ *Monitoramento ativado!*\n\n"
            f"üìå *Palavras-chave:* {len(palavras)}\n"
            f"üì° *Fontes:* {len(fontes)}\n"
            f"‚è±Ô∏è *Intervalo:* {MONITORAMENTO_INTERVAL//60} minutos\n"
            f"üîç *Filtro:* Not√≠cias dos √∫ltimos {DIAS_FILTRO_NOTICIAS} dias\n"
            f"üì∞ *Monitoramento:* Links espec√≠ficos de not√≠cias",
            parse_mode="Markdown"
        )

# Comando para ver status do monitoramento
async def status_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Exibe o status atual do monitoramento."""
    if not verificar_proprietario(update):
        await update.message.reply_text("‚ùå Acesso negado.")
        return
    
    config = carregar_config()
    monitoramento_ativo = config.get("monitoramento_ativo", False)
    ultima_verificacao = config.get("ultima_verificacao")
    
    palavras = config.get("palavras_chave", [])
    fontes = config.get("sites_monitorados", [])
    
    historico_links = len(config.get("historico_links", {}))
    
    status_emoji = "üü¢" if monitoramento_ativo else "üî¥"
    status_texto = "Ativo" if monitoramento_ativo else "Inativo"
    
    ultima_str = "Nunca"
    if ultima_verificacao:
        try:
            ultima_dt = datetime.fromisoformat(ultima_verificacao.replace('Z', '+00:00'))
            if ultima_dt.tzinfo is None:
                ultima_dt = TIMEZONE_BR.localize(ultima_dt)
            else:
                ultima_dt = ultima_dt.astimezone(TIMEZONE_BR)
            ultima_str = ultima_dt.strftime('%d/%m/%Y %H:%M:%S')
        except:
            ultima_str = "Erro na data"
    
    mensagem = (
        f"üìä *Status do Monitoramento*\n\n"
        f"{status_emoji} *Status:* {status_texto}\n"
        f"üìå *Palavras-chave:* {len(palavras)}\n"
        f"üì° *Fontes:* {len(fontes)}\n"
        f"‚è±Ô∏è *Intervalo:* {MONITORAMENTO_INTERVAL//60} minutos\n"
        f"üîç *Filtro:* √öltimos {DIAS_FILTRO_NOTICIAS} dias\n"
        f"üì∞ *Modo:* Links espec√≠ficos de not√≠cias\n"
        f"üìö *Hist√≥rico:* {historico_links} links\n"
        f"üïê *√öltima verifica√ß√£o:* {ultima_str}"
    )
    
    await update.message.reply_text(mensagem, parse_mode="Markdown")

# Exibir palavras-chave
async def ver_palavras(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Exibe todas as palavras-chave cadastradas."""
    if not verificar_proprietario(update):
        await update.message.reply_text("‚ùå Acesso negado.")
        return
    
    palavras = config_data.get("palavras_chave", [])
    if palavras:
        texto = "üìå *Palavras-chave cadastradas:*\n\n" + "\n".join([f"‚Ä¢ {palavra}" for palavra in palavras])
    else:
        texto = "üìå *Palavras-chave:*\n\nNenhuma palavra-chave cadastrada."
    
    await update.message.reply_text(texto, parse_mode="Markdown")

# Exibir fontes/perfis
async def ver_perfis(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Exibe todas as fontes e perfis cadastrados."""
    if not verificar_proprietario(update):
        await update.message.reply_text("‚ùå Acesso negado.")
        return
    
    sites = config_data.get("sites_monitorados", [])
    twitter = config_data.get("perfis_twitter", [])
    instagram = config_data.get("perfis_instagram", [])
    
    texto = "üì° *Fontes cadastradas:*\n\n"
    
    if sites:
        texto += "üåê *Sites:*\n" + "\n".join([f"‚Ä¢ {site}" for site in sites]) + "\n\n"
    
    if twitter:
        texto += "üê¶ *Twitter/X:*\n" + "\n".join([f"‚Ä¢ {perfil}" for perfil in twitter]) + "\n\n"
    
    if instagram:
        texto += "üì∏ *Instagram:*\n" + "\n".join([f"‚Ä¢ {perfil}" for perfil in instagram]) + "\n\n"
    
    if not (sites or twitter or instagram):
        texto += "Nenhuma fonte cadastrada."
    
    await update.message.reply_text(texto, parse_mode="Markdown")

def verificar_proprietario(update: Update) -> bool:
    """Verifica se o usu√°rio √© o propriet√°rio do bot."""
    user_id = update.message.from_user.id
    return config_data.get("telegram_owner_id") == user_id

# Adicionar ou remover termos via mensagens
async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Processa mensagens para adicionar ou remover termos."""
    if not verificar_proprietario(update):
        await update.message.reply_text("‚ùå Acesso negado.")
        return

    text = update.message.text.strip()
    
    if not text.startswith(("@", "#")):
        await update.message.reply_text(
            "‚ÑπÔ∏è Use @ para adicionar ou # para remover termos.\n"
            "Exemplo: @termo1, termo2"
        )
        return
    
    termos = [t.strip() for t in text[1:].split(",") if t.strip()]
    
    # Remover @ do in√≠cio de cada termo, caso o usu√°rio tenha colocado @ em cada URL
    termos = [termo.lstrip('@') for termo in termos]
    
    if not termos:
        await update.message.reply_text("‚ùå Nenhum termo v√°lido encontrado.")
        return

    if text.startswith("@"):
        # Adicionar termos
        for termo in termos:
            if termo.startswith("http"):
                # √â uma URL
                if "twitter.com" in termo or "x.com" in termo:
                    if termo not in config_data["perfis_twitter"]:
                        config_data["perfis_twitter"].append(termo)
                elif "instagram.com" in termo:
                    if termo not in config_data["perfis_instagram"]:
                        config_data["perfis_instagram"].append(termo)
                else:
                    # Site comum
                    if termo not in config_data["sites_monitorados"]:
                        config_data["sites_monitorados"].append(termo)
            else:
                # √â uma palavra-chave
                if termo not in config_data["palavras_chave"]:
                    config_data["palavras_chave"].append(termo)
        
        salvar_config(config_data)
        await update.message.reply_text(f"‚úÖ {len(termos)} termo(s) adicionado(s) com sucesso!")

    elif text.startswith("#"):
        # Remover termos
        removidos = 0
        for termo in termos:
            if termo in config_data["palavras_chave"]:
                config_data["palavras_chave"].remove(termo)
                removidos += 1
            elif termo in config_data["sites_monitorados"]:
                config_data["sites_monitorados"].remove(termo)
                removidos += 1
            elif termo in config_data["perfis_twitter"]:
                config_data["perfis_twitter"].remove(termo)
                removidos += 1
            elif termo in config_data["perfis_instagram"]:
                config_data["perfis_instagram"].remove(termo)
                removidos += 1
        
        if removidos > 0:
            salvar_config(config_data)
            await update.message.reply_text(f"‚úÖ {removidos} termo(s) removido(s) com sucesso!")
        else:
            await update.message.reply_text("‚ùå Nenhum dos termos especificados foi encontrado.")

def main():
    """Fun√ß√£o principal do bot."""
    global monitor_manager
    
    # Verificar se o token do bot est√° configurado
    bot_token = os.getenv("BOT_TOKEN")
    if not bot_token:
        logger.error("‚ùå Vari√°vel de ambiente BOT_TOKEN n√£o definida!")
        raise ValueError("‚ùå Vari√°vel de ambiente BOT_TOKEN n√£o definida!")
    
    # Inicializar o gerenciador de monitoramento
    monitor_manager = MonitoramentoManager(bot_token)
    
    # Criar aplica√ß√£o do bot
    application = ApplicationBuilder().token(bot_token).build()
    
    # Registrar handlers
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(CommandHandler("start", start_command))
    application.add_handler(CommandHandler("verificar", verificar_command))
    application.add_handler(CommandHandler("monitoramento", monitoramento_command))
    application.add_handler(CommandHandler("status", status_command))
    application.add_handler(CommandHandler("verpalavras", ver_palavras))
    application.add_handler(CommandHandler("verperfis", ver_perfis))
    application.add_handler(CommandHandler("limparhistorico", limpar_historico_command))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    
    logger.info("‚úÖ Faro Fino v2.7 iniciado com sucesso.")
    
    # Verificar se o monitoramento deve ser iniciado automaticamente
    config = carregar_config()
    if config.get("monitoramento_ativo", False):
        logger.info("üîÑ Monitoramento estava ativo, reiniciando...")
        monitor_manager.iniciar_monitoramento()
        # Criar task em background usando o contexto da aplica√ß√£o
        application.create_task(monitor_manager.loop_monitoramento())
    
    # Iniciar o bot usando run_polling sem asyncio.run
    application.run_polling(drop_pending_updates=True)

if __name__ == "__main__":
    main()

